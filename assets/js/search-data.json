{
  
    
        "post0": {
            "title": "The Naivete of Naive Bayes",
            "content": "So I finally figured out how to render LaTeX equations on my blog, and thus I can use my truly ‚Äúuseful‚Äù LaTex knowledge to start a new series on something that truly interests me. Machine Learning and how it works using the relative math in a simpler (and hopefully fun) language, to not scare away people who don‚Äôt prefer maths. Welcome to my series Machine Learning, with the Maths! (It sounds way cooler in my head!) . The point of this series is not to be a substitute for the plethora of tutorials already available on the topic of Machine Learning. I will try to explain the basic concepts behind each machine learning algorithm, and try to implement it from scratch using Python and NumPy. Maybe you can and hopefully will find better resources, but I hope this can be a good start for you. Plus, where else can I use my superbly huge and comically large Latex equations. Let‚Äôs begin, this probably very long post. . I am planning to write an introductory post on Bayes theorem anyways, so I am gonna skip the gory details here and link it in this post when I do end up writing it. Here‚Äôs the short intro to Bayes theorem. . There are a lot of ways to define what Bayes theorem represents. It can be thought of way to ‚Äúreverse‚Äù conditional probabilities. The interpretation we will be going through is that it can be used to find the probability of an event occurring, given the probability of another event that has already occurred. We can use (fun?) letters to denote some events, but hey, this is a Machine Learning post, so lets talk like that. Say you have a hypothesis (or an assumption) and you want to find the probability of that assumption, given the data. That is actually what Bayes theorem gives! Don‚Äôt believe me?. Let‚Äôs look at the equations . . This says that the gives the probability of event E occuring, given some event F occurs. Say event E represents our hypothesis and F represents the probability of obsering the data we observe. Then P(E | F) basically represents how much the data supports our assumption. Hence many times, Bayes theorem is also written like this | . . The term P(Hypothesis) is also called prior or prior beliefs. Why prior? It is the degree of the conviction that the hypothesis is true before we observe the actual data. Remember, prior is the sometimes the trickiest terms to determine (as mentioned later here). The term P(Hypothesis | Data) is called the posterior probability, which represents ‚Äúthe possibility of the hypothesis, given the data‚Äù. Posterior is usually build after seeing the data. The term P(Data | Hypothesis) is a fun one. It represents the probability of having obtained the data, given the hypothesis and is called the likelihood term. In that way, P(Data) is the Evidence we have, or the data we have. | . The evidence term can be broken as . P(F)=P(F‚à£E)P(E)+P(F‚à£E‚Ä≤)P(E‚Ä≤)P(F) = P(F|E)P(E) + P(F|E&amp;#x27;)P(E&amp;#x27;)P(F)=P(F‚à£E)P(E)+P(F‚à£E‚Ä≤)P(E‚Ä≤) . as well, which might come in handy later. . How Bayes Theorem is relevant to Machine Learning? . What Bayes theorem gives us is a framework to update our beliefs using evidence. Say you have a prior belief that some event occurs. Then you receive some evidence. You weigh your prior using the likelihood of that evidence happening and get a new belief, the posterior! Now when you get some more new evidence, you replace your prior belief with the posterior found about the event, and get a new posterior belief. And hey if you are not a probability nerd, let me explain this to you in a simpler way. Say you are a man who have lived his whole life inside a cave, and never got out. Now once you get out and see the sun rising on the east, what do you believe? Does it always happen or is this a one off? You assign some ‚Äúprior‚Äù probability of the sun rising in the east. The next day, the sun rises in east again and you update your belief using this evidence. Hmm, sun might rise on east on Mondays and Tuesdays and again update your belief (posterior). The more evidence you gather, the more you are sure about an event, and frankly that‚Äôs the crux of Machine Learning as well. We use data, or evidence, to update our beliefs about something. . Now with that out of the way, let‚Äôs turn to the example and actually implement this. . Spam me not! . The most common use of Naive Bayes, is for spam filters. Let‚Äôs look into how we can implement a spam filter using Naive Bayes from scratch. . Sometime ago, there was a scam on twitter where accounts of famous people like Elon Musk, Jeff Bezos etc. were hacked for ‚Äúbitcoin‚Äù scams, and let‚Äôs try to build a spam filter to help the poor souls who actually sent bitcoins to this scam. Let S be the event that the message is spam and B be the event that the message contains the word ‚Äòbitcoin‚Äô. Baye‚Äôs theorem tells us that the conditional probability that the message is spam, given it contains the word bitcoin is: . . The numerator is the probability that a message is spam and contains bitcoin, while the denominator is just the probability that a message contains bitcoin. How? P(B | S)P(S) = P(B,S) using the definition of conditional probablity (on which I did write a fun post you can read here) and the denominator is essentially P(B). In this sense, we can think of this calculation simply representing the proportion of bitcoin messages are spam. | . Say we have a large number of messages that we know are spam and a large collection of messages that we know are ham (the word used for not spam commonly). Using that we can easily estimate P(B/S) and P(B/S‚Äô). Let‚Äôs make an assumption that it is equally likely that a message is spam or ham. Then P(S) = P(S‚Äô) = 0.5. Taking out the common term in the above equation, we get . . Now say we find out that 50% of the spam messages contain the word bitcoin but only 1% of non spam messages do, then the probablity that any given bitcoin-containing message is spam is: . 0.5/(0.5 + 0.01) = 98% . Making the Spam Filter more Sophisticated . Let‚Äôs bring in equations now (this aint called Machine Learning with Maths for no reason). Imagine we have a vocabulary of many words w1, w2, ‚Ä¶ ,wn and we say event Xi means the message contains the word wi. Also, since we are imagining so much, imagine that we have some undisclosed process to get an estimate of P(Xi/S) and P(Xi/S‚Äô), or basically the probability that a spam/not spam message contains the ith word. . Why is this Bayes Naive? . The key to Naive Bayes is making the (big) assumption that the presences (or absences) of each word are independent of one another, conditional on a message being spam or not. Intuitively, this assumption means that knowing whether a certain spam message contains the word bitcoin gives you no information about whether that same message contains the word rolex. In terms of ML terminologies, it makes the assumption that features of a measurement are independent of each other. . In math terms, this assumption means that: . . Now I am not going to lie, this is an extreme assumption, and the reason why it is called Naive Bayes. Say our vocabulary contains only the words ‚Äúbitcoin‚Äù and ‚Äúgold‚Äù and that half the messages that are spam are for ‚Äúearn bitcoin‚Äù and the other half messages that are spam are for ‚Äú26kt gold‚Äù. Thus P(Bitcoin/Spam) = 0.5 and P(Gold/Scam) = 0.5 as well. The probability that a message is spam which contains both ‚Äúbitcoin‚Äù and ‚Äúgold‚Äù is P(Bitcoin, Gold/Spam) = P(Bitcoin/Spam) * P(Gold/Spam) = 0.5 x 0.5 = 0.25. This happens since we assumed away the knowledge that ‚Äúbitcoin‚Äù and ‚Äúgold‚Äù never occur together. . The fun part is that, despite the unrealisticness of this assumption, this model often performs well and has historically been used in actual spam filters. . Using the same equation that we used for the bitcoin only spam filter, we can calculate the probability of a message being spam using the following equation: . . The Naive Bayes assumption allows us to compute each of the probabilities on the right simply by multiplying together the individual probability estimates for each vocabulary word and hence simplifies our calculation. . A practical consideration . In order to prevent underflow, where in computers don‚Äôt do well with floating-point numbers that are too close to 0, we try to avoid multiplying probability values. Instead of that, we can use log to multiply probabilities using log(ab) = log(a) + log(b) and then do an exp(logx) = x to get back the actual probability. This doesn‚Äôt change any of the equations or assumptions, it is just a small practical trick to avoid getting weird answers. . Training? . Now the only problem is estimating P(Xi/S) and P(Xi/S‚Äô), the probabilities that a spam message (or nonspam message) contains the word wi. If we have a fair number of ‚Äútraining‚Äù messages labeled as spam and not spam, an obvious first try is to estimate P(Xi/S) simply as the fraction of spam messages containing the word wi. . Being Smooth . While this calculation seems reasonable, it has a huge problem. Say, in our training messages, the word ‚Äúdata‚Äù only occurs in nonspam messages. Then we‚Äôd estimate P(data | S)=0. The result is that our Naive Bayes classifier would always assign spam probability 0 to any message containing the word data, even a message like ‚Äúdata on free bitcoin and 26 kt gold free.‚Äù To avoid this, we usually use some kind of smoothing. One of the simplest ways is to choose a pseudo count k (basically assuming that there are atleast k spam/ham messages containing the given word i). This gives us the following equation for estimating the probability of seeing the ith word in a spam or ham message as follows: | . . We can do similarly for P(Xi | S‚Äô) where in we assume we also saw k additional nonspams containing the word and k additional nonspams not containing the word. | . For example, if data occurs in 0/98 spam messages, and if k is 1, we estimate P(data | S) as 1/100 = 0.01, which allows our classifier to still assign some nonzero spam probability to messages that contain the word data. | . Code Implementation . Now that we have all the pieces to build our classifier, the only thing to do is to actually build our classifier. This post contains code snippets but you can find the code on my GitHub repo here . Since we are planning to deal with text data with this classifier, we will be needing to tokenize our text to words/tokens. We assume we have a simple function tokenize() that returns all the tokens in a sentence. (A simple implementation for this would be to convert all text to lower case and then use regular expressions to remove special characters like apostrophes). We can obviously have complex tokenizing pipelines and more text pre-processings, but we will skip that for now. . Our training data consists of the message and a boolean is_spam indicating whether the message is spam?. We implement our classifier as a class to use it in a better way. The constructor only takes the parameter k. We also initialize a set to contain unique tokens, dictionaries which are counters to track how often each token is seen in spam messages and ham messages, and the counts of how many spam and ham messages it was trained on. . class NaiveBayesClassifier: def __init__(self, k = 0.5): self.k = k self.tokens = set() self.token_spam_counts = defaultdict(int) self.token_ham_counts = defaultdict(int) self.spam_messages = self.ham_messages = 0 . Now to implement the train function for it. According to the message type, we first increment the counts of spam or ham messages. Then we tokenize the message and then increment the spam/ham counts for each token. . def train(self, messages): for message in messages: if message.is_spam: self.spam_messages += 1 else: self.ham_messages += 1 # Increment word counts for token in tokenize(message.text): self.tokens.add(token) if message.is_spam: self.token_spam_counts[token] += 1 else: self.token_ham_counts[token] += 1 . Ultimately we‚Äôll want to predict P(spam | token). As we saw earlier, to apply Bayes‚Äôs theorem we need to know P(token / spam) and P(token / ham) for each token in the vocabulary. So we‚Äôll create a helper function to compute those. (_probabilities) | . def _probabilities(self, token): &quot;&quot;&quot; Returns P(token|spam) and P(token|ham) &quot;&quot;&quot; spam = self.token_spam_counts[token] ham = self.token_ham_counts[token] p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k) p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k) return p_token_spam, p_token_ham . Finally we can write a predict method, and we will use the method of summing logs (as mentioned in here). This takes a message and tokenizes it. Then using the helper function it finds the probabilities of each token in the vocabulary. Then adds/multiplies the respective probability of seeing/not seeing the token in te message and then returns the probability of the given message being spam or ham. . def predict(self, text): text_tokens = tokenize(text) log_prob_if_spam = log_prob_if_ham = 0.0 # Iterate through each word in vocabulary for token in self.tokens: prob_if_spam, prob_if_ham = self._probabilities(token) # If token appears in message # add the log probability of seeing it if token in text_tokens: log_prob_if_spam += math.log(prob_if_spam) log_prob_if_ham += math.log(prob_if_ham) # Otherwise add the log probability # of not seeing it which is # log(1-probability of seeing it) else: log_prob_if_spam += math.log(1-prob_if_spam) log_prob_if_ham += math.log(1-prob_if_ham) prob_if_spam = math.exp(log_prob_if_spam) prob_if_ham = math.exp(log_prob_if_ham) . Inspecting the Model . We can even have a helper function that ‚Äúinspect‚Äù the model‚Äôs innards see which words are indicative of spam/not spam. . def p_spam_given_token(token, model): prob_if_spam, prob_if_ham = model._probabilities(token) return prob_if_spam/(prob_if_spam + prob_if_ham) words = sorted(model.tokens, key=lambda t: p_spam_given_token(t,model)) print(&quot;Spammiest Words: &quot;, words[-10:]) print(&quot;Hammiest Words: &quot;, words[:10]) . Trying out the model on a dataset . We will run this model on the UCI ML SMS Spam Dataset which can be found here. For the detailed code, you can visit this notebook in my GitHub repo . The model returns ‚Äúclaim‚Äù ‚Äúprize‚Äù as words indicating the message is spam which is a good sign of this performing okay. . Possible Improvements . How could we get better performance? One obvious way would be to get more data to train on. There are a number of ways to improve the model as well. Here are some possibilities that you might try: . Our classifier takes into account every word that appears in the training set, even words that appear only once. Modify the classifier to accept an optional min_count threshold and ignore tokens that don‚Äôt appear at least that many times. | The tokenizer has no notion of similar words (e.g., cheap and cheapest). Modify the classifier to take an optional stemmer function that converts words to equivalence classes of words. | Although our features are all of the form ‚Äúmessage contains word wi,‚Äù there‚Äôs no reason why this has to be the case. In our implementation, we could add extra features like ‚Äúmessage contains a number‚Äù by creating phony tokens like contains:number and modifying the tokenizer to emit them when appropriate. | . The horrors of prior . After all that code, there is one small topic I would like to touch on, which is on estimating priors. . Prior is one of the trickiest terms to determine in the Bayes equation. As explained really nicely in this video by Veritasium, Bayes theorem tells us how to update our beliefs in light of new evidence. It cant tell us how to set our prior beliefs. So it is possible for someone to have a different prior belief than other, because they are subjective. Some people might be more certain about a prior belief that other people. That‚Äôs how bias can creep in. And we definitely need a world, and a model, with lesser bias. . Mathematically, instead of ‚Äúchoosing‚Äù a prior, we assume the prior probability to follow a prior model and we try to estimate these model parameters (for example assuming the prior distribution follows Gaussian distribution and finding its parameters) . For further exploration . You can visit this blog for another ‚Äúfrom scratch‚Äù implementation of Naive Bayes or this blog as well . . The idea and much of the code was from the book ‚ÄúData Science from Scratch‚Äù which is truly an amazing read for someone who wants to implement stuff from scratch. . If you stuck with me till this long, it seems you enjoyed this post. I hope to keep updating this new Machine Learning, with the Maths series so keep an eye on this blog. You can always @ me at my socials or the GitHub repo for this blog. Thanks for reading. üòä .",
            "url": "https://mitesh1612.github.io/blog/probability/bayes/python/2020/08/30/naive-bayes.html",
            "relUrl": "/probability/bayes/python/2020/08/30/naive-bayes.html",
            "date": " ‚Ä¢ Aug 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "C# Properties and their different Uses",
            "content": "I learnt C# in parts from various resources, mostly because each one had their different starting points. One book heavily relied on using Visual Studio which definitely makes life easy, but makes you ignore the nitties-gritties of setting up .NET projects, while one did everything from scratch, which did scratch my itch (Get it? üòú) but had a really really slow pace. . One constructs that I learnt about while seeing how to create classes was C# Properties. Properties have a little confusing syntax as described here (in all the non syntax highlighted glory of C#) . public class MyRiches { // Normal Data Members public int Money; // Properties public int GoldInKgs { get; set; } } . If you come from some other language, upon seeing this, you might be like . When I learnt about properties, I kept wondering why do I work extra hard to define data members when I can do it in a simpler and a much known way. I though it was just syntactic sugar, only with its visible diabetes as well. Then I saw an interesting usage of properties in our existing codebase, something like this . public class MyClass { public IList&lt;SomeType&gt; Property { get { if(m_property == null) { m_property = new List&lt;SomeType&gt;(); } return m_property; } set { m_property = value; } } private IList&lt;SomeType&gt; m_property; } . and I suddenly doubting my already dubious C# knowledge. Thus I began a journey on to understand the actual use and various usage of patterns. (Don‚Äôt worry, I will also explain this example soon) . So what actually are Properties? . Before getting into why these exist, lets assume they didnt. Now say I had a data member in my class that was private but I needed to access it‚Äôs value in some other class, but not let it modify the value. For example . public class Bank { private int AccountBalance; } public class Me { var myBankAccount = new Bank(); var myMoney = myBankAccount.AccountBalance; } . Now if I cant directly access the AccountBalance property. If I make it public, although it might be fun for me, it might not be fun for the bank, when I could just do this . // The easy way of becoming a millionaire myBankAccount.AccountBalance = 1000000; . So what is the other solution? Ah yes, getters (and setters, their siblings). We could easily define a getter on this data member that will get us the value but not let modify it. Something like this: . public class Bank { private int AccountBalance; public int getAccountBalance() { return AccountBalance; } } public class Me { var myBankAccount = new Bank(); var myMoney = myBankAccount.getAccountBalance(); } . If you are used to working in Java with Eclipse, you know it has a functionality of auto generating getters and setters and I‚Äôm partly sure the creators of C# might have already loaded this in Visual Studio, but for people who didnt use it, this was a long and tedious methods to write this getters and setters. That‚Äôs why, C# creators created properties. . Properties provide an access mechanism for private data members . So even though you can essentially create data members using properties, that‚Äôs not their intended use. Essentially Properties provide a flexible mechanism to read, write or compute the value of a private field. They can be used as data members, but they are actually special methods called accessors, which as you guessed it, are useful for accessing data. . If you are not a fan of big words, this basically means, they are a shortcut for writing customized getter and setter methods. Thus, in a way, they indeed are syntactic sugar, without the harms? I am not going to bore you with the syntax of C# Properties, here is a good reference. . Now comes the fun part, the various usage patterns of Properties. The get and set aren‚Äôt just for show. You can customize them to implement various fun and useful patterns in your code, and here I‚Äôll show a few . Lazy Loading Values using Properties . Properties can help you implement a cache with lazy loading feature. For example . private int m_IncomeTax; public int IncomeTax { get { if(m_IncomeTax == null) { m_IncomeTax = AReallyLongComputationForTax(); } return m_IncomeTax; } } . This is the other primary usage of Properties, of course other than controlling access. . Future Proofing Code . Say you want to maintain the API of your class but the logic or calculation changes. To incorporate that without affecting your class API, you can change the setter code. . In terms of the above example, say in the computation for tax, they include a cess (YES! Tax on Tax!), you can change the getters like this, such that IncomeTax property gives you the total tax always. . // Old public int IncomeTax { get { return m_IncomeTax; } } // New public int IncomeTax { get { return m_IncomeTax + Cess; } } . Creating a Contract . Properties help you create a contract/API for a class. This way you can have a proper contract of accessing class members, which might be private or calculations on some private members. They are useful if you need some extra calculations on private members. . In general, the point is to separate implementation (the field) from API (the property). Later on you can, should you wish, put logic, logging etc in the property without breaking either source or binary compatibility - but more importantly you‚Äôre saying what your type is willing to do, rather than how it‚Äôs going to do it. More on this in this article . Returning Non Null Values using Properties . I promised I‚Äôll explain the code at the beginning of the section. For reference, this is the code . public class MyClass { public IList&lt;SomeType&gt; Property { get { if(m_property == null) { m_property = new List&lt;SomeType&gt;(); } return m_property; } set { m_property = value; } } private IList&lt;SomeType&gt; m_property; } . This code essentially checks whether a given member is null, and if it is null, it will populate the value first, and then return it. There is one great benefit of using this approach. Anything that consumes the value of this property need not have a null check, since this essentially ensures that you never get a null value and reduces the amount of code and checks you need to write, and don‚Äôt we all want to write less code? . In closing, I know properties seem like glorified setters/getters and all of the benefits mentioned above can also be done using setters and getters as well, as said above, they are just syntactic sugar. Learning how they can be used to control access rather than being used as public data members can help flesh out some nice code as well. . An honourable TypeScript Mention . Typescript itself has a similar method of implementing getters on private variables, which I remembered when I was reading up on properties. This looks quite similar to the C# Properties implementation and hence the mention here. . class MyClass { private _property; public get property() { return _property; } } var data = new MyClass(); var value = data.property; .",
            "url": "https://mitesh1612.github.io/blog/c%23/properties/classes/2020/08/19/c-sharp-properties.html",
            "relUrl": "/c%23/properties/classes/2020/08/19/c-sharp-properties.html",
            "date": " ‚Ä¢ Aug 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Conditional Probability and Families",
            "content": "So I was reading the book ‚ÄúData Science from Scratch‚Äù by Joel Grus. I came across this nice tricky problem in Conditional Probability mentioned in this book. Let‚Äôs take a look . A Quick Refresher on Conditional Probability . Well if you forgot the definition of conditional probability, here is a small refresher for you. Say there are two events E and F. The probability that E happens given that we know F happens is represented using . P(E‚à£F)P(E|F)P(E‚à£F) . Mathematically, . P(E‚à£F)=P(E,F)/P(F).P(E,F)P(E|F) = P(E,F)/P(F). P(E,F)P(E‚à£F)=P(E,F)/P(F).P(E,F) . is the probability of both E and F happening. . Well enough of a probability class, lets look at an interesting family now . The Family and The Unknown Children . Say there is a family with two unknown children. If we assume that: . Each child is equally likely to be a boy or a girl | The gender of the second child is independent of the first child. | . Then the event ‚Äúno girls‚Äù has probability 1/4, the event ‚Äúone girl, one boy‚Äù has probability 1/2, and the event ‚Äútwo girls‚Äù has probability 1/4. . Now here comes the interesting problem. ‚ÄúWhat is the probability of the event ‚Äòboth children are girls‚Äô (B) conditional on the event that ‚Äòthe older child is a girl (G)‚Äô?‚Äù . This aint a test, so here is how we can easily calculate this using conditional probability . P(B‚à£G)=P(B,G)/P(G)P(B|G) = P(B,G)/P(G)P(B‚à£G)=P(B,G)/P(G) . T event B and G (‚Äúboth children are girls and the older child is a girl‚Äù) is just the event B. (Once you know that both children are girls, it‚Äôs necessarily true that the older child is a girl). Thus, . P(B‚à£G)=P(B,G)/P(G)=P(B)/P(G)=1/2P(B|G) = P(B,G)/P(G) = P(B)/P(G) = 1/2P(B‚à£G)=P(B,G)/P(G)=P(B)/P(G)=1/2 . This is mostly intuitive. Now can you guess ‚ÄúWhat is the probability of the event ‚Äòboth children are girls‚Äô (B) conditional on the event that ‚Äòat least one of the children is a girl‚Äô (L)?‚Äù . Surprisingly, the answer to this question is different from the one before. Here is how . As before, the event B and L (‚Äúboth children are girls and at least one of the children is a girl‚Äù) is just the event B. This means we have: . P(B‚à£L)=P(B,L)/P(L)=P(B)/P(L)=1/3P(B|L) = P(B,L)/P(L) = P(B)/P(L) = 1/3P(B‚à£L)=P(B,L)/P(L)=P(B)/P(L)=1/3 . How can this be the case? Well, if all you know is that at least one of the children is a girl, then it is twice as likely that the family has one boy and one girl than that it has both girls. . Still don‚Äôt believe me? . Well if you are the doubting kind, we can write a simple Python code to simulate this experiment. . import enum, random class Kid(enum.Enum): BOY = 0 GIRL = 1 def random_kid() -&gt; Kid: return random.choice([Kid.BOY, Kid.GIRL]) both_girls = 0 older_girl = 0 either_girl = 0 random.seed(20200814) # Yep, this post&#39;s date for _ in range(10000): younger = random_kid() older = random_kid() if older == Kid.GIRL: older_girl += 1 if older == Kid.GIRL and younger == Kid.GIRL: both_girls += 1 if older == Kid.GIRL or younger == Kid.GIRL: either_girl += 1 print(&quot;P(both | older):&quot;, both_girls / older_girl) # 0.494 ~ 1/2 print(&quot;P(both | either): &quot;, both_girls / either_girl) # 0.322 ~ 1/3 . This indicates how the conditioning of an event affects its probability. . Weirdly, this problem reminds me a lot of the Monty Hall Problem. Hey if you have any explaination on how this can relate to Monty Hall, you can always @ me at my socials (linked in this blog at various locations!üòâ) and I‚Äôll probably try to figure out if there is any relation between these two (after I refresh my Probability Course üòã) .",
            "url": "https://mitesh1612.github.io/blog/probability/python/2020/08/14/conditional-probability.html",
            "relUrl": "/probability/python/2020/08/14/conditional-probability.html",
            "date": " ‚Ä¢ Aug 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Constructing a complex dictionary of base and derived class using the same code",
            "content": "Okay, really complex title aside, I‚Äôll try to explain the problem I had and the interesting way I found to solve it. This may not be the best or the most optimized way, but I really liked this solution and would like to share it. . Also, although I do prefer Python, this post is in C#, since that was the language I encountered this problem in. . So here is the problem. Say we have two C# Classes . public class BaseClass { public string SomeProperties { get; set; } } public class DerivedClass : BaseClass { public string SomeOtherProperties { get; set; } } . I wanted to construct a Dictionary of these classes like Dictionary&lt;string, BaseClass&gt; and Dictionary&lt;string, DerivedClass&gt; at two very different places. The construction of each dictionary element was not that trivial due to the inherent complexity of filling the properties for both the classes. Here is an example of how one of the dictionaries was being created: . // For the base class var map = new Dictionary&lt;string, BaseClass&gt;(); foreach(var someProperty in someList) { var baseElement = new BaseClass(someProperty); map[someProperty] = baseElement; } foreach(var dependency in dependencyList) { map[dependency.To] = map[dependency.From] } // For the derived class var map = new Dictionary&lt;string, DerivedClass&gt;(); foreach(var someProperty in someList) { // someOtherProperty comes from somewhere else var derivedElement = new DerivedClass(someProperty, someOtherProperty); map[someProperty] = derivedElement; } foreach(var dependency in dependencyList) { map[dependency.To] = map[dependency.From] } . As you can see, there is a lot of logic repeating since the properties that were created for the base class were also created for the derived class, but when we create the DerivedClass object, new properties were also to be added to those object. Both the objects differ in how they are constructed but the way the map is created is similar. I wanted a way to reuse these for loops instead of writing them for both BaseClass and DerivedClass and other classes that might inherit from BaseClass later. . My basic idea was to use a Template method like this. . public Dictionary&lt;string, &lt;T&gt;&gt; CreateDictionary(parameters) { while(someConditionOnParameters) { if(T is BaseClass) { // Base class object creation code } else if(T is DerivedClass) { // Derived class object creation code } // repeated dictionary creation code } } . The other problem that I encounter here in was the arguments to this method. When we create the BaseClass object, I require fewer properties but when I created DerivedClass object, I require more properties and hence the number and type of arguments couldn‚Äôt be fixed. Of course, I can set/pass them as null and ignore when not needed, but that didn‚Äôt feel like a tidy solution to me. Plus later on, when we derive a new class from BaseClass, again the signature of method changes which might break a few things here and there. . That‚Äôs when I was suggested the interesting solution to this problem, the one I am going to share now. We keep one function that creates this dictionary but rather than passing the parameters to create the objects, we pass a function that creates those objects for us. For example when we want to create the BaseClass dictionary, we can pass a function that creates the base class object and so on. This way this method can be extensible for any classes that derive from future as well. Here is a dummy code to show how that method might look like . public Dictionary&lt;string, BaseClass&gt; CreateDictionary(DataObject requiredData, Func&lt;Data, BaseClass&gt; objectCreator) { var map = new Dictionary&lt;string, BaseClass&gt;(); foreach(Data propertyValues in requiredData.data) { var element = objectCreator(Data); } foreach(var dependency in requiredData.dependencies) { map[dependency.To] = dependency.From; } } . Now when I want to create the base class dictionary, I can call it like: . var map = CreateDictionary(requiredData, x =&gt; { return new BaseClass(x.somePropertyValue); }); . Or if I want the derived class dictionary, like this: . var map = CreateDictionary(requiredData, x =&gt; { return new DerivedClass(x.SomePropertyValue, someOtherPropertyValue); }); . I really loved this solution, its nifty and useful and this didnt come to my mind easily. . Closing Thoughts . I know this is a really specific and weird problem to encounter, and some constraints of why this solution was used over other ways are not clear from the vague names and class designs (and possibly incomplete details) I provided. However, I really found the solution interesting and felt like sharing it. . You can always share your thoughts on this by @‚Äôing me on Twitter or LinkedIn (links are available in my author bio) or even this blog‚Äôs GitHub repo. .",
            "url": "https://mitesh1612.github.io/blog/c%23/dictionaries/objects/2020/08/13/constructing-objects.html",
            "relUrl": "/c%23/dictionaries/objects/2020/08/13/constructing-objects.html",
            "date": " ‚Ä¢ Aug 13, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Welcome to this blog",
            "content": "Welcome to this blog . Hey everyone, welcome to my blog. I finally took the leap and set up my blog, and after a lot of thinking, I finally decided to create a blog. . Why a blog? Well among a lot of other reasons, like not forgetting what I learn from time to time, and to keep track of my journey as a Software Development Engineer. The other huge reason is that I love to learn new things, research the ones I find interesting, and I plan to document and share my learnings via blog posts, which can be easily found and referred to later as well. I‚Äôll try to share things I learn about Software Development, my interests in Data Science and all the other random things I encounter. Hopefully, other people will also find these posts helpful, relevant or interesting. You can always share your views on my GitHub repo here (until I figure out adding comment sections to a static GitHub site üòâ) . Thanks for visiting this blog! . About Me . I‚Äôm Mitesh Shah, and I live in Hyderabad. I started my journey as a Software Engineer at Microsoft (where I currently work). I have a lot of interest in Data Science and Machine Learning and I love reading books or playing some games in my relaxing time. . Technical Details for this Blog . This blog is possible due to Gatsby JS with the wonderful theme Novela created by the Narative Team. Right now this site is hosted on GitHub pages using a CI from GitHub Actions . Update : 24-12-2020 . Since then, I have moved this blog to Fastpages. The links in this post are updated now. .",
            "url": "https://mitesh1612.github.io/blog/introduction/2020/08/12/welcome-to-this-blog.html",
            "relUrl": "/introduction/2020/08/12/welcome-to-this-blog.html",
            "date": " ‚Ä¢ Aug 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hey there. I am the writer and maintainer of this blog, Mitesh Shah. Right now I work as a Software Engineer in Microsoft and love to read up on various topics like Data Science, Machine Learning, Backend Development in my free time. Feel free to connect with my on my socials. .",
          "url": "https://mitesh1612.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "An Example Markdown Post",
          "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
          "url": "https://mitesh1612.github.io/blog/demo-post.txt",
          "relUrl": "/demo-post.txt",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://mitesh1612.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}